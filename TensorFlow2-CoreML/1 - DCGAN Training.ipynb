{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2 GAN -> CoreML Notebook, Part 1\n",
    "\n",
    "This notebook trains a TensorFlow 2 color image generative adversarial network.\n",
    "\n",
    "Also included are some fun plotting functions for exploring / animating the latent space.\n",
    "\n",
    "## Dependencies, Installation and Environment\n",
    "To run this you'll need a Python 3 environment with **TensorFlow 2**:\n",
    "- tensorflow or tensorflow-gpu 2+ (GPU highly recommended. I'm currently running tensorflow-gpu version 2.2.0)\n",
    "- matplotlib\n",
    "- numpy\n",
    "- jupyter\n",
    "\n",
    "If you do use a GPU, you can test it by running:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "```\n",
    "\n",
    "## The Model, Training, and Configuration\n",
    "This model is going to run on a wide variety of iPhones, so for simplicity we'll start with a deep convolutional network ([DCGAN](https://www.tensorflow.org/tutorials/generative/dcgan)).\n",
    "\n",
    "There are a lot of known issues that arise training GANs, but in these experiments I've been mostly concerned with (1) image quality and (2) \"mode collapse\".\n",
    "\n",
    "Image quality is pretty subjective. I'm looking to make pictures that are *close enough* to input images, and which are free of any weird artifacts or patterns (e.g. [checkerboarding](https://distill.pub/2016/deconv-checkerboard/)).\n",
    "\n",
    "Mode collapse occurs when the generator overfits and starts producing the same output over and over.\n",
    "\n",
    "I'm generating artsy images from very small datasets: currently on the order of 500-20000 training samples. For my purposes, I've found that the models defined below tend to reach a happy place around 200-400 training epochs. You'll need to inspect the generated images and see what works for you.\n",
    "\n",
    "This is a simple notebook, and there's much, much more that could be done to improve training performance. Some next steps might include changing up the loss function, or adding evaluation metrics like FrÃ©chet Inception Distance. For more detailed writeups, see: [Common Problems (with GANs)](https://developers.google.com/machine-learning/gan/problems), [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628), and [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks).\n",
    "\n",
    "If you're looking to incorporate labels or want some control over inputs in the latent space, check out Conditional GAN (CGAN) and Information Maximizing GAN (InfoGAN).\n",
    "\n",
    "## Getting started\n",
    "The basic architecture and training sections of this notebook are based on the [\"GAN overriding Model.train_step\"](https://keras.io/examples/generative/dcgan_overriding_train_step/) tutorial, which trains a deep convolutional GAN with TensorFlow 2 + Keras to generate black and white digits based on the MNIST dataset.\n",
    "\n",
    "The model has been modified to output 128x128 RGB images. Read along for more details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Pick a model name\n",
    "Files for this model will be saved to **./models/{model_name}**\n",
    "\n",
    "## Add training data\n",
    "Add image paths to \"data_paths\" to include them in training. If you're looking to include labels, modify the process_path and/or Dataset initializer.\n",
    "\n",
    "I tested this out on my own pictures of flowers. For something similar, check out the [Oxford Flowers 17](http://www.robots.ox.ac.uk/~vgg/data/flowers/17/), [TensorFlow Flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers), or [Oxford Flowers 102](https://www.tensorflow.org/datasets/catalog/oxford_flowers102).\n",
    "\n",
    "### Image size, etc.\n",
    "You can also change the shape of latent input, or the size of output images here. **If you change the image output size, be prepared to modify the generator model to match it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General settings and image preprocessing helpers\n",
    "model_name = 'MODEL_NAME'\n",
    "data_paths = ['./PATH_TO_IMAGES/*']\n",
    "latent_dim = 128\n",
    "img_shape = (128, 128, 3)\n",
    "\n",
    "def decode_img(img, img_size=(128, 128)):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Convert image to floats in [0,1]\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize and convert to [-1, 1]\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    img = (img - 0.5) * 2.0\n",
    "    return img\n",
    "\n",
    "def process_path(file_path):\n",
    "    # load the raw data from the file as a string\n",
    "    img = decode_img(tf.io.read_file(file_path))\n",
    "    return img\n",
    "\n",
    "filenames = []\n",
    "for idx, path in enumerate(data_paths):\n",
    "    batch_ds = tf.data.Dataset.list_files(path)\n",
    "    batch_filenames = list(batch_ds)\n",
    "    filenames.extend(batch_filenames)\n",
    "    print(\"{}: {} images\".format(path, len(batch_filenames)))\n",
    "\n",
    "file_ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "all_images = file_ds.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = all_images.shuffle(buffer_size=1024).batch(64).prefetch(32).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "The model layers are all specified with Keras. Some comments:\n",
    "    - Use LeakyReLU in discriminator and generator\n",
    "    - Initialize transpose convolutions with random noise\n",
    "    - Make sure that transpose convolutions are formatted with color channels last\n",
    "    - Use BatchNormalization in generator, and tanh for final activation\n",
    "\n",
    "As of the time of writing, there was no out-of-the-box CoreML support for a Spectral Normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=img_shape),\n",
    "        layers.Conv2D(32, (5, 5), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,), name='noise_in'),\n",
    "\n",
    "        layers.Dense(16 * 16 * 256, use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((16, 16, 256)),\n",
    "        layers.UpSampling2D(),\n",
    "        \n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1),\n",
    "                               padding=\"same\",\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                               data_format=\"channels_last\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        \n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2),\n",
    "                               padding=\"same\",\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                               data_format=\"channels_last\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        \n",
    "        layers.Conv2DTranspose(32, (5, 5), strides=(2, 2),\n",
    "                               padding=\"same\",\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                               data_format=\"channels_last\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        \n",
    "        layers.Conv2D(3, (5, 5), strides=(1, 1), activation='tanh', padding='same')\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "\n",
    "generator_model.summary()\n",
    "\n",
    "# Make sure the generator's output shape matches the image shape we're expecting\n",
    "assert(generator_model.layers[-1].output_shape[1:] == img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This is pretty much unmodified from the source notebook.\n",
    "\n",
    "The GANMonitor class is a callback that saves a grid of generated images at the end of each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "        \n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This callback plots a grid of images generated from a random sampling of points in the latent space\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, img_path, img_rows=4, img_cols=4, latent_dim=128):\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_path = img_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.img_rows * self.img_cols, self.latent_dim))\n",
    "\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images = 0.5 * generated_images + 0.5\n",
    "        generated_images = generated_images.numpy()\n",
    "        \n",
    "        raw_vectors = random_latent_vectors.numpy()\n",
    "        \n",
    "        fig, axs = plt.subplots(self.img_rows, self.img_cols, figsize=(15,15), gridspec_kw={'wspace': 0.025, 'hspace': 0.025})\n",
    "        fig.suptitle('Epoch {}'.format(epoch), fontsize=16)\n",
    "        cnt = 0\n",
    "        for i in range(self.img_rows):\n",
    "            for j in range(self.img_cols):\n",
    "                axs[i,j].imshow(generated_images[cnt])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        outfile = os.path.join(self.img_path, '{}.png'.format(epoch))\n",
    "        fig.savefig(outfile)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup\n",
    "\n",
    "All model files are saved in **./models/{model_name}**. The particulars are:\n",
    " - weights.hdf5: trained weights\n",
    " - generator.json: generator layer architecture\n",
    " - discriminator.json: discrinimator layer architecture\n",
    " - images/{epoch}.png: evaluation images generated at the end of each epoch\n",
    "\n",
    "**If existing model definition files or weights are found, they're loaded automatically**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for this model\n",
    "# The folder will contain model architecture definitions, trained weights, and evaluation images\n",
    "model_path = os.path.join('models', model_name)\n",
    "weights_path = os.path.join(model_path, 'weights.hdf5') # Combined model weights\n",
    "generator_weights_path = os.path.join(model_path, 'generator_weights.hdf5')\n",
    "img_path = os.path.join(model_path, 'images')\n",
    "mlmodel_path = os.path.join(model_path, \"model.mlmodel\")\n",
    "generator_path = os.path.join(model_path, 'generator_architecture.json')\n",
    "discriminator_path = os.path.join(model_path, 'discriminator_architecture.json')\n",
    "\n",
    "print(\"Working in {}\".format(model_path))\n",
    "\n",
    "if not os.path.exists(img_path):\n",
    "    os.makedirs(img_path)\n",
    "    \n",
    "if os.path.exists(generator_path):\n",
    "    with open(generator_path, 'r') as fp:\n",
    "        generator = keras.models.model_from_json(fp.read())\n",
    "        print(\"Loaded generator architecture from {}\".format(generator_path))\n",
    "        print(generator.summary())\n",
    "else:\n",
    "    generator = generator_model\n",
    "    print(\"Using new generator\")\n",
    "    with open(generator_path, 'w') as fp:\n",
    "        fp.write(generator.to_json())\n",
    "        \n",
    "if os.path.exists(discriminator_path):\n",
    "    with open(discriminator_path, 'r') as fp:\n",
    "        discriminator = keras.models.model_from_json(fp.read())\n",
    "        print(\"Loaded discriminator architecture from {}\", discriminator_path)\n",
    "        print(discriminator.summary())\n",
    "else:\n",
    "    discriminator = discriminator_model\n",
    "    print(\"Using new discriminator\")\n",
    "    with open(discriminator_path, 'w') as fp:\n",
    "        fp.write(discriminator.to_json())\n",
    "\n",
    "# Two Time-Scale Update Rule: settings discriminator LR higher than the generator\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0004, beta_1=0.5),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "plot_images = GANMonitor(img_path, latent_dim=latent_dim)\n",
    "\n",
    "# Save weights every epoch. Monitoring Inception Score / FID could be a nice improvement\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(weights_path,\n",
    "                                             monitor='g_loss',\n",
    "                                             verbose=1,\n",
    "                                             save_best_only=False,\n",
    "                                             mode='auto',\n",
    "                                             save_freq=1)\n",
    "\n",
    "if os.path.exists(weights_path):\n",
    "    print('Loading weights from {}'.format(weights_path))\n",
    "    gan.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 225\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[plot_images, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the generator weights\n",
    "These will get loaded elsewhere and included with the final CoreML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.generator.save_weights(generator_weights_path)\n",
    "print(\"Saved generator weights to {}\".format(generator_weights_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Utilities\n",
    "\n",
    "Use these cells to visualize output in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spherical linear interpolation for exploring latent space\n",
    "# Basically, since we're sampling the latent space with a Gaussian distribution, our sample points are closer to a hypersphere than a uniformly distributed hypercube. For more on this see:\n",
    "# https://github.com/soumith/dcgan.torch/issues/14\n",
    "# https://machinelearningmastery.com/how-to-interpolate-and-perform-vector-arithmetic-with-faces-using-a-generative-adversarial-network/\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    omega = np.arccos(np.clip(np.dot(low/np.linalg.norm(low), high/np.linalg.norm(high)), -1, 1))\n",
    "    so = np.sin(omega)\n",
    "    if so == 0:\n",
    "        # Becomes LERP when omega -> 0\n",
    "        return (1.0-val) * low + val * high\n",
    "    return np.sin((1.0-val)*omega) / so * low + np.sin(val*omega) / so * high\n",
    "\n",
    "def interpolate(a, b, n_steps=10):\n",
    "    steps = np.linspace(0, 1, num=n_steps)\n",
    "    vectors = []\n",
    "    return np.array([slerp(t, a, b) for t in steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid of random points in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same as in GANMonitor\n",
    "\n",
    "rows, cols = 4, 4\n",
    "random_latent_vectors = tf.random.normal(shape=(rows*cols, latent_dim))\n",
    "\n",
    "generated_images = gan.generator(random_latent_vectors)\n",
    "generated_images = 0.5 * generated_images + 0.5\n",
    "generated_images = generated_images.numpy()\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15,15), gridspec_kw={'wspace': 0.025, 'hspace': 0.025})\n",
    "cnt = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        axs[i,j].imshow(generated_images[cnt])\n",
    "        axs[i,j].axis('off')\n",
    "        cnt += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated interpolation between points in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.axis('off')\n",
    "imgs = []\n",
    "\n",
    "imobj = ax.imshow(np.zeros((128, 128)), origin='upper', alpha=1.0, zorder=1, aspect=1)\n",
    "\n",
    "count = 8\n",
    "\n",
    "points = tf.random.normal(shape=(count, latent_dim))\n",
    "for i in range(0, count-1):\n",
    "    interpolated = interpolate(points[i], points[i+1], n_steps=40)\n",
    "    generated_images = gan.generator(interpolated)\n",
    "    generated_images = 0.5 * generated_images + 0.5\n",
    "    imgs.extend(generated_images)\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(imgs):\n",
    "        img = imgs[i]\n",
    "        imobj.set_data(img)\n",
    "    return imobj,\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=len(imgs), repeat=True, interval=135, blit=True, repeat_delay=100)\n",
    "\n",
    "# Comment these lines in to save the animation as a gif\n",
    "writer = animation.PillowWriter(fps=12)\n",
    "anim.save(os.path.join(model_path, \"animation.gif\"), writer=writer)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment these lines in to save the animation as a gif\n",
    "writer = animation.PillowWriter(fps=24)\n",
    "anim.save(os.path.join(model_path, \"animation.gif\"), writer=writer)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid of interpolated points in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 8\n",
    "cols = 8\n",
    "\n",
    "points = tf.random.normal(shape=(rows+1, latent_dim))\n",
    "imgs = []\n",
    "for i in range(0, rows):\n",
    "    interpolated = interpolate(points[i], points[i+1], n_steps=cols)\n",
    "    generated_images = gan.generator(interpolated)\n",
    "    generated_images = 0.5 * generated_images + 0.5\n",
    "    imgs.extend(generated_images)\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(16, 16), gridspec_kw={'wspace': 0.025, 'hspace': 0.025})\n",
    "cnt = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        axs[i,j].imshow(imgs[cnt])\n",
    "        axs[i,j].axis('off')\n",
    "        cnt += 1\n",
    "fig.savefig(os.path.join(model_path, 'grid.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoreML Conversion\n",
    "\n",
    "For more about converting the trained generator to CoreML, check out my other notebook."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
