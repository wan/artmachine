{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2 + Keras Model -> CoreML\n",
    "\n",
    "This notebook loads weights trained in a TensorFlow 2 DCGAN model to a TensorFlow 1 + Keras model, and converts that to CoreML.\n",
    "\n",
    "## Installation / Environment\n",
    "\n",
    "You'll need a working Python 3 environment with **TensorFlow 1 and Keras** installed (and Jupyter, of course). For stability, I've chosen:\n",
    " - keras==2.2.4\n",
    " - tensorflow-gpu==1.13.1\n",
    " - coremltools==3.4\n",
    "\n",
    "## coremltools and TensorFlow 2\n",
    "[coremltools 4.0b1](https://github.com/apple/coremltools/releases) has improved TF 2 conversion support, but I've had issues with it. Could be user error, but redefining the model with TF 1 is easy enough.\n",
    "\n",
    "For more about coremltools 4.0, check out the [documentation here](https://coremltools.readme.io/docs).\n",
    "\n",
    "That said I'm optimistic about streamlined TF 2 conversion in the near future! Hopefully this entire notebook becomes uncessary soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.initializers import TruncatedNormal\n",
    "\n",
    "import os\n",
    "\n",
    "import coremltools as ct\n",
    "from coremltools.proto import FeatureTypes_pb2 as ft "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Redefinition\n",
    "This model is an exact redefinition an existing TensorFlow 2 model. It's the same architecture, but uses `keras` instead of `tensorflow.keras`.\n",
    "\n",
    "Input is a 128x1 vector, Output is a 128x128x3 image.\n",
    "\n",
    "### READ THE MODEL SUMMARY!\n",
    "The model summary should match the TF 2 model summary *exactly*. If not, the weights won't load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(256 * 16 * 16,\n",
    "                    use_bias=False,\n",
    "                    input_dim=latent_dim))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Reshape((16, 16, 256)))\n",
    "    model.add(layers.UpSampling2D())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1),\n",
    "                                     use_bias=False,\n",
    "                                     padding=\"same\",\n",
    "                                     kernel_initializer=TruncatedNormal(stddev=0.02),\n",
    "                                     data_format='channels_last'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2),\n",
    "                                     use_bias=False,\n",
    "                                     padding=\"same\",\n",
    "                                     kernel_initializer=TruncatedNormal(stddev=0.02),\n",
    "                                     data_format='channels_last'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2),\n",
    "                                     use_bias=False,\n",
    "                                     padding=\"same\",\n",
    "                                     kernel_initializer=TruncatedNormal(stddev=0.02),\n",
    "                                     data_format='channels_last'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(3, (5, 5), strides=(1, 1), activation='tanh', padding='same'))\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model weights\n",
    "\n",
    "In this example I'm loading the generator from my DCGAN notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MODEL_FOLDER'\n",
    "model_path = os.path.join('models', model_name)\n",
    "generator_weights_path = os.path.join(model_path, 'generator_weights.hdf5')\n",
    "mlmodel_path = os.path.join(model_path, 'Model.mlmodel')\n",
    "model.load_weights(generator_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoreML Conversion\n",
    "\n",
    "Use these cells to convert the model to CoreML using coremltools.\n",
    "\n",
    "For ease-of-use with CoreML, it's important to convert the output type from multiArrayType to imageType.\n",
    "\n",
    "Alternatively, you can convert MLMultiArray to an Image manually at inference time, but I wouldn't recommend it. There's much more on the subject by [Matthijs Hollemans](https://machinethink.net/blog/coreml-image-mlmultiarray/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel = ct.converters.keras.convert(\n",
    "    model,\n",
    "    input_names=[\"noise_in\"]\n",
    ")\n",
    "spec = mlmodel.get_spec()\n",
    "print(spec.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "The model currently outputs a 128x128x3 RGB array with values in [-1, 1]. Let's add some postprocessing to scale that output to [0, 255].\n",
    "\n",
    "For more on CoreML model i/o, see [How to convert images to MLMultiArray](https://machinethink.net/blog/coreml-image-mlmultiarray/) and [How to Get Core ML Proudce Images as Output](https://cutecoder.org/featured/core-ml-image-output/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a reference to the last layer\n",
    "spec_layers = getattr(spec, spec.WhichOneof(\"Type\")).layers\n",
    "last_layer = spec_layers[-1]\n",
    "\n",
    "# Create a new activation layer that scales output to [0, 255]\n",
    "new_layer = spec_layers.add()\n",
    "new_layer.name = 'image_out'\n",
    "new_layer.activation.linear.alpha = 127.5\n",
    "new_layer.activation.linear.beta = 127.5\n",
    "new_layer.output.append('image_out')\n",
    "\n",
    "# Point the last layer's output to the new layer\n",
    "new_layer.input.append(last_layer.output[0])\n",
    "\n",
    "# Find the original model's output description\n",
    "output_description = next(x for x in spec.description.output if x.name==last_layer.output[0])\n",
    "\n",
    "# Update the model output to use the new layer\n",
    "output = spec.description.output[0]\n",
    "output.name = new_layer.name\n",
    "output.type.imageType.colorSpace = ft.ImageFeatureType.RGB\n",
    "output.type.imageType.width = 128\n",
    "output.type.imageType.height = 128\n",
    "\n",
    "print(spec.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model = ct.models.MLModel(spec)\n",
    "updated_model.save(mlmodel_path)\n",
    "print(\"Saved mlmodel file to {}\".format(mlmodel_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model (in Swift)\n",
    "\n",
    "To use the model in your iOS project, start by adding it to your project.\n",
    "\n",
    "XCode will automatically generate a class for your model based on the mlmodel file. Click on the mlmodel file to see information about the generated class name. While you're there, confirm the input and output types of your model.\n",
    "\n",
    "**You may need to build once to silence XCode warnings**.\n",
    "\n",
    "The gist of things is that we'll be using MLMultiArray to represent latent vectors, with the goal of creating UIImage output that we can work with easily.\n",
    "\n",
    "To start, we can use [GKRandomDistribution](https://developer.apple.com/documentation/gameplaykit/gkgaussiandistribution) to generate normally distributed random vectors:\n",
    "```swift\n",
    "// Generate a noise vector (MLMultiArray) with n dimensions\n",
    "func createNoise(_ n: Int = 128, distribution: GKGaussianDistribution) -> MLMultiArray {\n",
    "    guard let mlArray = try? MLMultiArray(shape: [n as NSNumber], dataType: .float32) else {\n",
    "        fatalError(\"Unexpected runtime error. MLMultiArray\")\n",
    "    }\n",
    "    for idx in (0..<n) {\n",
    "        mlArray[idx] = distribution.nextUniform() as NSNumber\n",
    "    }\n",
    "    return mlArray\n",
    "}\n",
    "```\n",
    "\n",
    "If you need random numbers from a different mean / standard deviation, check out [this answer on Stackoverflow](https://stackoverflow.com/a/49471411).\n",
    "\n",
    "The autogenerated CoreML model class will generate CVPixelBuffer output. There are a [few ways](https://machinethink.net/blog/coreml-image-mlmultiarray/) to get that into a UIImage. I've been using a small UIImage Extension:\n",
    "```swift\n",
    "import VideoToolbox\n",
    "\n",
    "extension UIImage {\n",
    "    public convenience init?(pixelBuffer: CVPixelBuffer) {\n",
    "        var cgImage: CGImage?\n",
    "        VTCreateCGImageFromCVPixelBuffer(pixelBuffer, options: nil, imageOut: &cgImage)\n",
    "\n",
    "        if let cgImage = cgImage {\n",
    "            self.init(cgImage: cgImage)\n",
    "        } else {\n",
    "            return nil\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, we can instantiate the model and noise distribution, call prediction() with the a noise vector, and turn the result into a UIImage:\n",
    "```swift\n",
    "import CoreML\n",
    "import GameKit\n",
    "\n",
    "var model = flowers_v1() // autogenerated model class\n",
    "let random = GKRandomSource()\n",
    "let noiseDistribution = GKGaussianDistribution(randomSource: random, mean: 0, deviation: 1)\n",
    "\n",
    "// Create a noise vector and run a prediction\n",
    "let ganInput = createNoise(128, distribution: noiseDistribution)\n",
    "guard let transformed = try? model.prediction(noise_in: ganInput) else {\n",
    "    print(\"Failed to predict\")\n",
    "}\n",
    "let ganImage = UIImage(pixelBuffer: transformed.image_out)!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
